import os
import json
import requests
from tqdm import tqdm
from concurrent.futures import ThreadPoolExecutor, as_completed

NEWS_API_URL = "https://apiradar.finup.co.kr/App"
HEADERS = {
    "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) Chrome/120.0.0.0 Safari/537.36",
    "Content-Type": "application/json"
}

INPUT_PATH = "./data/stock_list_with_newsKeywordIdx.json"
OUTPUT_DIR = "./news_data"
MAX_WORKERS = 10  # ë™ì‹œì— ì‹¤í–‰í•  ìŠ¤ë ˆë“œ ìˆ˜
os.makedirs(OUTPUT_DIR, exist_ok=True)


def fetch_news_by_keyword_idx(news_idx):
    all_news = []
    page = 1

    while True:
        body = {
            "ApiID": "ALARM_HISTORY_KEYWORD",
            "ApiGB": "ALARM",
            "KeywordIdx": int(news_idx),
            "PageNo": page,
            "PageSize": 30,
            "TypeFilter": "10"
        }
        try:
            print(f"\nğŸ“¡ [ìš”ì²­] KeywordIdx={news_idx}, Page={page}")
            res = requests.post(NEWS_API_URL, headers=HEADERS, json=body, timeout=10)
            print(f"ğŸ” ìƒíƒœì½”ë“œ: {res.status_code}")
            data = res.json()
            items = data.get("Result", [])[0] if isinstance(data.get("Result"), list) else []
            print(f"ğŸ“„ ë‰´ìŠ¤ ê°œìˆ˜: {len(items)}")

            if not items:
                print(f"ğŸ›‘ ë” ì´ìƒ ë‰´ìŠ¤ ì—†ìŒ (í˜ì´ì§€ {page})")
                break

            all_news.extend(items)
            page += 1
        except Exception as e:
            print(f"âŒ [KeywordIdx: {news_idx}] ìš”ì²­ ì‹¤íŒ¨: {e}")
            break

    return all_news


def save_news_jsonl(news_list, path, keyword_idx):
    with open(path, "w", encoding="utf-8") as f:
        for item in news_list:
            if not isinstance(item, dict):
                continue

            title = item.get("Title")
            # content = item.get("Content") or item.get("Summary")
            # if not title or not content:
            #     continue

            record = {
                "keyword": item.get("Keyword", ""),  # âœ… ì¢…ëª©ëª…
                "title": title,
                "summary": item.get("Summary"),
                "date": item.get("PublishDT", "")[:10],
                "url": item.get("Url"),
                "media": item.get("MediaName", ""),    # âœ… ì–¸ë¡ ì‚¬
                "keywordIdx": keyword_idx
            }
            f.write(json.dumps(record, ensure_ascii=False) + "\n")


def crawl_single(stock):
    name = stock["name"]
    code = stock["code"]
    news_idx = stock.get("newsKeywordIdx")

    if not news_idx:
        return f"âš ï¸  {name} ({code}) â†’ newsKeywordIdx ì—†ìŒ"

    print(f"\nğŸš€ {name} ({code}) ë‰´ìŠ¤ ìˆ˜ì§‘ ì‹œì‘ â†’ newsKeywordIdx: {news_idx}")
    file_path = os.path.join(OUTPUT_DIR, f"{code}.jsonl")
    news_items = fetch_news_by_keyword_idx(news_idx)

    if news_items:
        save_news_jsonl(news_items, file_path, news_idx)
        return f"âœ… {name} ({code}) ë‰´ìŠ¤ {len(news_items)}ê±´ ì €ì¥"
    else:
        return f"âš ï¸  {name} ({code}) ë‰´ìŠ¤ ì—†ìŒ"


def crawl_news_all():
    with open(INPUT_PATH, "r", encoding="utf-8") as f:
        stock_list = json.load(f)

    results = []
    with ThreadPoolExecutor(max_workers=MAX_WORKERS) as executor:
        futures = [executor.submit(crawl_single, stock) for stock in stock_list]
        for future in tqdm(as_completed(futures), total=len(futures), desc="ğŸ“° ë³‘ë ¬ ë‰´ìŠ¤ ìˆ˜ì§‘ ì¤‘"):
            result = future.result()
            results.append(result)
            print(result)


if __name__ == "__main__":
    crawl_news_all()
